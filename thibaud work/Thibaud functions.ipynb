{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from proj1_helpers import load_csv_data, predict_labels, create_csv_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_mse(y, tx, w):\n",
    "    \"\"\"Calculate the loss using mse.\"\"\"\n",
    "    N = y.shape[0]\n",
    "    e = y - tx @ w.T\n",
    "    return 1 / (2 * N) * np.linalg.norm(e) ** 2\n",
    "\n",
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    N = y.shape[0]\n",
    "    e = y - tx @ w.T\n",
    "    return -1 / N * tx.T @ e \n",
    "\n",
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Least squares using gradient descent algorithm.\"\"\"\n",
    "    w = initial_w\n",
    "    \n",
    "    for _ in range(max_iters):\n",
    "        DL = compute_gradient(y, tx, w)        \n",
    "        w = w - DL * gamma\n",
    "        \n",
    "    return w, compute_mse(y, tx, w)\n",
    "\n",
    "def least_squares_SGD(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"Least squares using stochastic gradient descent algorithm.\"\"\"\n",
    "    w = initial_w\n",
    "    \n",
    "    for _ in range(max_iters):        \n",
    "        for yn, txn in batch_iter(y, tx, batch_size):\n",
    "            DL_n = compute_stoch_gradient(yn, txn, w)\n",
    "            w = w - DL_n * gamma\n",
    "                        \n",
    "    return w, compute_mse(y, tx, w)\n",
    "\n",
    "def least_squares(y, tx):\n",
    "    \"\"\"Least squares using normal equation.\"\"\"\n",
    "    a = tx.T @ tx\n",
    "    b = tx.T @ y\n",
    "    w = np.linalg.solve(a, b)\n",
    "    return w, compute_mse(y, tx, w)\n",
    "\n",
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"Ridge regression.\"\"\"\n",
    "    N = tx.shape[0]\n",
    "    a = (tx.T @ tx) + 2 * N * lambda_ * np.eye(tx.shape[1])\n",
    "    b = tx.T @ y\n",
    "    w = np.linalg.solve(a, b)\n",
    "    return w, compute_mse(y, tx, w)\n",
    "\n",
    "def sigmoid(t):\n",
    "    \"\"\"Sigmoid function on t.\"\"\"\n",
    "    return 1 / (1 + np.exp(-t))\n",
    "\n",
    "def calculate_loss(y, tx, w):\n",
    "    \"\"\"Cost by negative log likelihood.\"\"\"\n",
    "    sigma_tx_w = sigmoid(tx @ w)\n",
    "    sum_terms = y * np.log(sigma_tx_w) + (1 - y) * np.log(sigma_tx_w)\n",
    "    return -sum_terms.sum()\n",
    "\n",
    "def calculate_gradient(y, tx, w):\n",
    "    \"\"\"Gradient of loss.\"\"\"\n",
    "    tx_w = tx @ w\n",
    "    sigma_tx_w = sigmoid(tx_w)\n",
    "    # print('simga_tx_w - y')\n",
    "    # print(sigma_tx_w - y)\n",
    "    grad = tx.T @ (sigma_tx_w - y)\n",
    "    # print('tx.T')\n",
    "    # print(tx.T.shape)\n",
    "    return grad\n",
    "\n",
    "import time\n",
    "\n",
    "def logistic_regression(y, tx, initial_w, max_iters, gamma):\n",
    "    '''Logistic regression.'''\n",
    "    w = initial_w\n",
    "    \n",
    "    for _ in range(max_iters):\n",
    "        # time.sleep(3) \n",
    "        grad = calculate_gradient(y, tx, w)\n",
    "        #print('grad')\n",
    "        #print(grad)\n",
    "        # print('w')\n",
    "        # print(w)\n",
    "        w = w - gamma * grad\n",
    "        \n",
    "    return w, calculate_loss(y, x, w)\n",
    "\n",
    "def reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma):\n",
    "    w = initial_w\n",
    "    \n",
    "    for _ in range(max_iters):\n",
    "        grad = calculate_gradient(y, tx, w) + lambda_ * np.sum(w)\n",
    "        w = w - gamma * grad\n",
    "        \n",
    "    return w, calculate_loss(y, tx, w)\n",
    "\n",
    "def standardize(x):\n",
    "    \"\"\"Standardize the original data set.\"\"\"\n",
    "    mean_x = np.mean(x, axis=0)\n",
    "    x = x - mean_x\n",
    "    std_x = np.std(x, axis=0)\n",
    "    x = x / std_x\n",
    "    return x, mean_x, std_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(x, y, ratio, seed=None):\n",
    "    \"\"\"\n",
    "    split the dataset based on the split ratio. If ratio is 0.8 \n",
    "    you will have 80% of your data set dedicated to training \n",
    "    and the rest dedicated to testing\n",
    "    \"\"\"\n",
    "    # set seed\n",
    "    if not seed is None:\n",
    "        np.random.seed(seed)\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # split the data based on the given ratio: TODO\n",
    "    # ***************************************************\n",
    "    d = x.shape[0]\n",
    "    di = int(d * ratio)\n",
    "    \n",
    "    per = np.random.permutation(d)\n",
    "    \n",
    "    xtraining = x[per][:di]\n",
    "    ytraining = y[per][:di]\n",
    "    xtesting = x[per][di:]\n",
    "    ytesting = y[per][di:]\n",
    "    \n",
    "    return xtraining, ytraining, xtesting, ytesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_poly(x, degree):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # polynomial basis function: TODO\n",
    "    # this function should return the matrix formed\n",
    "    # by applying the polynomial basis to the input data\n",
    "    # ***************************************************\n",
    "    phi = np.ones((x.shape[0], 1))\n",
    "    for deg in range(1, degree+1):\n",
    "        phi = np.c_[phi, x ** deg]\n",
    "    return phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validation(y, x, k_indices, k, lambda_, degree):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # get k'th subgroup in test, others in train: TODO\n",
    "    # ***************************************************\n",
    "    train_indices = np.delete(k_indices, k, 0).flatten()\n",
    "    test_indices = k_indices[k]\n",
    "    \n",
    "    x_train = x[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    x_test = x[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # form data with polynomial degree: TODO\n",
    "    # ***************************************************\n",
    "    phi_train = build_poly(x_train, degree)\n",
    "    phi_test = build_poly(x_test, degree)\n",
    "    \n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # ridge regression: TODO\n",
    "    # ***************************************************\n",
    "    w_opt, _ = ridge_regression(y_train, phi_train, lambda_)\n",
    "    \n",
    "    # print(w_opt)\n",
    "    \n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # calculate the loss for train and test data: TODO\n",
    "    # ***************************************************\n",
    "    loss_tr = np.sqrt(2 * compute_mse(y_train, phi_train, w_opt))\n",
    "    loss_te = np.sqrt(2 * compute_mse(y_test, phi_test, w_opt))\n",
    "    \n",
    "    return w_opt, loss_tr, loss_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from plots import cross_validation_visualization\n",
    "\n",
    "def cross_validation_demo(y, x):\n",
    "    seed = 1\n",
    "    degree = 3\n",
    "    k_fold = 4\n",
    "    lambdas = np.logspace(-10, 2, 50)\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    # define lists to store the loss of training data and test data\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # cross validation: TODO\n",
    "    # ***************************************************\n",
    "    tr = np.zeros([len(lambdas), k_fold])\n",
    "    te = np.zeros([len(lambdas), k_fold])\n",
    "    \n",
    "    for ind, lambda_ in enumerate(lambdas):\n",
    "        for k in range(k_fold):\n",
    "            _, loss_tr, loss_te = cross_validation(y, x, k_indices, k, lambda_, degree)\n",
    "            tr[ind, k] = loss_tr\n",
    "            te[ind, k] = loss_te\n",
    "            \n",
    "    rmse_tr = np.mean(tr, axis=1)\n",
    "    rmse_te = np.mean(te, axis=1)\n",
    "            \n",
    "    cross_validation_visualization(lambdas, rmse_tr, rmse_te)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "# Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# yb, input_data, ids = load_csv_data('C:/Users/Thibaud/Documents/data/train.csv', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yb_full, input_data_full, ids_full = load_csv_data('C:/Users/Thibaud/Documents/data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yb_test, input_data_test, ids_test = load_csv_data('C:/Users/Thibaud/Documents/data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "yb, input_data, ids = yb_full[::10], input_data_full[::10], ids_full[::10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000,), (25000, 30), (25000,))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb.shape, input_data.shape, ids.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cross_validation(yb, input_data, build_k_indices(yb, 5, 3), 0, 0.1, 3)\n",
    "cross_validation_demo(yb, input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "------------------------\n",
    "# Treating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_nans_with_mean(arr, nan=-999):\n",
    "    '''Creates a copy and replaces the nan values by the mean (without those nan values) in the column'''\n",
    "    N, D = arr.shape\n",
    "    copy = arr.copy()\n",
    "    \n",
    "    for d in range(D):\n",
    "        copy[:,d][copy[:,d] == nan] = np.mean(arr[:,d][arr[:,d] != nan])\n",
    "        \n",
    "    return copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_nans_with_most_frequent(arr, nan=-999):\n",
    "    '''Creates a copy and replaces the nan values by the most frequent value in the column'''\n",
    "    N, D = arr.shape\n",
    "    copy = arr.copy()\n",
    "    \n",
    "    for d in range(D):\n",
    "        unique, counts = np.unique(arr[:,d], return_counts=True)\n",
    "        copy[:,d][copy[:,d] == nan] = unique[np.argmax(counts[unique != nan])]\n",
    "        \n",
    "    return copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_nans_with_median(arr, nan=-999):\n",
    "    '''Creates a copy and replaces the nan values by the median (without thos nan values) in the column'''\n",
    "    N, D = arr.shape\n",
    "    copy = arr.copy()\n",
    "    \n",
    "    for d in range(D):\n",
    "        copy[:,d][copy[:,d] == nan] = np.median(arr[:,d][arr[:,d] != nan])\n",
    "        \n",
    "    return copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prediction(w, x_test, y_test, small=-1, big=1, verbose=False):\n",
    "    y_pred = x_test @ w\n",
    "    sep_val = (small + big) / 2\n",
    "    y_pred[y_pred < sep_val] = small\n",
    "    y_pred[y_pred >= sep_val] = big\n",
    "    \n",
    "    bad = np.count_nonzero(y_pred - y_test)\n",
    "    good = y_test.shape[0] - bad\n",
    "    \n",
    "    ratio = good / (good + bad)\n",
    "    \n",
    "    if verbose:\n",
    "        print('Good: ', good)\n",
    "        print('Bad: ', bad)\n",
    "        print('Ratio: ', ratio)\n",
    "    \n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actually no good ideas $\\downarrow$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice idea $\\downarrow$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_nans = replace_nans_with_mean(input_data)\n",
    "no_nans_std, _, _ = standardize(no_nans)\n",
    "\n",
    "x_train, y_train, x_test, y_test = split_data(no_nans_std, yb, 0.5)\n",
    "\n",
    "phi = build_poly(x_train, 7)\n",
    "\n",
    "w, _ = least_squares(y_train, phi)\n",
    "prediction(w, build_poly(x_test, 7), y_test, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better idea $\\downarrow$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_nans = replace_nans_with_median(input_data)\n",
    "no_nans_std, _, _ = standardize(no_nans)\n",
    "\n",
    "y = (yb + 1) / 2\n",
    "\n",
    "x_train, y_train, x_test, y_test = split_data(no_nans_std, y, 0.5)\n",
    "\n",
    "phi = build_poly(x_train, 7)\n",
    "\n",
    "w, _ = least_squares(y_train, phi)\n",
    "prediction(w, build_poly(x_test, 7), y_test, small=0, big=1, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bad idea $\\downarrow$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_col = add_extra_col(input_data)\n",
    "no_nans = replace_nans_with_mean(extra_col)\n",
    "no_nans_std, _, _ = standardize(no_nans)\n",
    "\n",
    "x_train, y_train, x_test, y_test = split_data(no_nans_std, y, 0.5)\n",
    "\n",
    "phi = build_poly(x_train, 7)\n",
    "w, _ = least_squares(y_train, phi)\n",
    "prediction(w, build_poly(x_test, 7), y_test, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better idea $\\downarrow$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_with_nans = np.any(input_data == -999, axis=1).astype(int)\n",
    "no_nans = replace_nans_with_mean(input_data)\n",
    "no_nans_std, _, _ = standardize(no_nans)\n",
    "concat = np.c_[no_nans_std, rows_with_nans]\n",
    "\n",
    "x_train, y_train, x_test, y_test = split_data(concat, yb, 0.5)\n",
    "\n",
    "phi = build_poly(x_train, 7)\n",
    "\n",
    "w, _ = ridge_regression(y_train, phi, 0.00001)\n",
    "prediction(w, build_poly(x_test, 7), y_test, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "deg_min = 1\n",
    "deg_max = 9\n",
    "degrees = np.linspace(deg_min, deg_max, deg_max - deg_min + 1).astype(int)\n",
    "ratios = []\n",
    "losses_tr = []\n",
    "losses_te = []\n",
    "\n",
    "extra_col = np.any(input_data == -999, axis=1).astype(int)\n",
    "no_nans = replace_nans_with_median(input_data)\n",
    "no_nans_std, _, _ = standardize(no_nans)\n",
    "x_all = np.c_[no_nans_std, extra_col]\n",
    "\n",
    "x_train, y_train, x_test, y_test = split_data(no_nans, yb, 0.9)\n",
    "\n",
    "for degree in degrees:\n",
    "    phi_train = build_poly(x_train, degree)\n",
    "    phi_test = build_poly(x_test, degree)\n",
    "    \n",
    "    w, loss = ridge_regression(y_train, phi_train, 0.00001)\n",
    "    \n",
    "    losses_tr.append(np.sqrt(compute_mse(y_train, phi_train, w)))\n",
    "    losses_te.append(np.sqrt(compute_mse(y_test, phi_test, w)))\n",
    "    \n",
    "    ratios.append(prediction(w, phi_test, y_test))\n",
    "    \n",
    "sns.plt.plot(degrees, ratios)\n",
    "sns.plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.plt.plot(degrees, losses_tr)\n",
    "sns.plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.plt.plot(degrees, losses_te)\n",
    "sns.plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_deg = degrees[np.argmax(ratios)]\n",
    "degrees[np.argmax(ratios)], np.max(ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lambda_min = -15\n",
    "lambda_max = 0\n",
    "lambdas = np.logspace(lambda_min, lambda_max, lambda_max - lambda_min + 1)\n",
    "ratios = []\n",
    "losses_tr = []\n",
    "losses_te = []\n",
    "'''\n",
    "extra_col = np.any(input_data == -999, axis=1).astype(int)\n",
    "no_nans = replace_nans_with_median(input_data)\n",
    "no_nans_std, _, _ = standardize(no_nans)\n",
    "x_all = np.c_[no_nans_std, extra_col]\n",
    "\n",
    "x_train, y_train, x_test, y_test = split_data(no_nans_std, yb, 0.5)\n",
    "'''\n",
    "for lambda_ in lambdas:\n",
    "    phi_train = build_poly(x_train, best_deg)\n",
    "    phi_test = build_poly(x_test, best_deg)\n",
    "    \n",
    "    w, loss = ridge_regression(y_train, phi_train, lambda_)\n",
    "    \n",
    "    losses_tr.append(np.sqrt(compute_mse(y_train, phi_train, w)))\n",
    "    losses_te.append(np.sqrt(compute_mse(y_test, phi_test, w)))\n",
    "    \n",
    "    ratios.append(prediction(w, phi_test, y_test))\n",
    "    \n",
    "sns.plt.semilogx(lambdas, ratios)\n",
    "sns.plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.plt.semilogx(lambdas, losses_tr)\n",
    "sns.plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.plt.semilogx(lambdas, losses_te)\n",
    "sns.plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas[np.argmax(ratios)], np.max(ratios)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# extra_col = np.any(input_data == -999, axis=1).astype(int)\n",
    "input_data_clean = replace_nans_with_median(np.delete(input_data, [22, 29], 1))\n",
    "input_data_std, _, _ = standardize(input_data_clean)\n",
    "\n",
    "times = 1000\n",
    "degrees = np.linspace(3, 5, 3).astype(int) # [1, 2, 3]\n",
    "lambdas = np.logspace(-5, 0, 30) # [0.00001, 0.0001, 0.001, 0.01, 0.1, 1]\n",
    "\n",
    "rmse_tr = np.zeros([times, len(degrees), len(lambdas)])\n",
    "rmse_te = np.zeros([times, len(degrees), len(lambdas)])\n",
    "bests = np.zeros(len(degrees))\n",
    "\n",
    "for time in range(times):\n",
    "    x_tr, y_tr, x_te, y_te = split_data(input_data_std, yb, 0.8, seed=time)\n",
    "    # x_tr, y_tr, x_te, y_te, per, di = split_data_2(input_data_std, yb, 0.8, seed=time)\n",
    "    \n",
    "    # extra_col_tr = extra_col[per][:di]\n",
    "    # extra_col_te = extra_col[per][di:]\n",
    "\n",
    "    for i, degree in enumerate(degrees):\n",
    "        phi_tr = build_poly(x_tr, degree)\n",
    "        phi_te = build_poly(x_te, degree)\n",
    "        # phi_tr = np.c_[build_poly(x_tr, degree), extra_col_tr]\n",
    "        # phi_te = np.c_[build_poly(x_te, degree), extra_col_te]\n",
    "\n",
    "        for j, lambda_ in enumerate(lambdas):\n",
    "            w, _ = ridge_regression(y_tr, phi_tr, lambda_)\n",
    "\n",
    "            rmse_tr[time, i, j] = np.sqrt(2 * compute_mse(y_tr, phi_tr, w))\n",
    "            rmse_te[time, i, j] = np.sqrt(2 * compute_mse(y_te, phi_te, w))\n",
    "            \n",
    "    if time == 0.1 * times:\n",
    "        print('10%')\n",
    "    if time == 0.2 * times:\n",
    "        print('20%')\n",
    "    if time == 0.3 * times:\n",
    "        print('30%')\n",
    "    if time == 0.4 * times:\n",
    "        print('40%')\n",
    "    if time == 0.5 * times:\n",
    "        print('50%')\n",
    "    if time == 0.6 * times:\n",
    "        print('60%')\n",
    "    if time == 0.7 * times:\n",
    "        print('70%')\n",
    "    if time == 0.8 * times:\n",
    "        print('80%')\n",
    "    if time == 0.9 * times:\n",
    "        print('90%')\n",
    "        \n",
    "\n",
    "_, ax = sns.plt.subplots(len(degrees), 1, figsize=(8, len(degrees) * 20 / 3))\n",
    "        \n",
    "for i, degree in enumerate(degrees):\n",
    "    ax[i].semilogx(lambdas, rmse_tr.mean(axis=0)[i], color='b', marker='o')\n",
    "    ax[i].semilogx(lambdas, rmse_te.mean(axis=0)[i], color='r', marker='o')\n",
    "    ax[i].set_xlabel(\"lambda\")\n",
    "    ax[i].set_ylabel(\"RMSE\")\n",
    "    ax[i].set_title(\"degree \" + str(degree) + \" :\")\n",
    "    \n",
    "    bests[i] = lambdas[np.argmin(rmse_te.mean(axis=0)[i])]\n",
    "    \n",
    "sns.plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "# Separating data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data separated by the value in column 22 which is a categorical column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_by_col22(x, idd, y):    \n",
    "    x_22 = [np.delete(x[x[:,22] == i], 22, 1) for i in range(4)]\n",
    "    idd_22 = [idd[x[:,22] == i] for i in range(4)]\n",
    "    y_22 = [y[x[:,22] == i] for i in range(4)]\n",
    "    \n",
    "    return x_22, idd_22, y_22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_by_22, ids_by_22, yb_by_22 = separate_by_col22(input_data, ids, yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing the eprcentages of -999 in each column of the separated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "   0 : 0.2672499502883277\n",
      "   4 : 1.0\n",
      "   5 : 1.0\n",
      "   6 : 1.0\n",
      "   12 : 1.0\n",
      "   22 : 1.0\n",
      "   23 : 1.0\n",
      "   24 : 1.0\n",
      "   25 : 1.0\n",
      "   26 : 1.0\n",
      "   27 : 1.0\n",
      "1\n",
      "   0 : 0.10520246027678114\n",
      "   4 : 1.0\n",
      "   5 : 1.0\n",
      "   6 : 1.0\n",
      "   12 : 1.0\n",
      "   25 : 1.0\n",
      "   26 : 1.0\n",
      "   27 : 1.0\n",
      "2\n",
      "   0 : 0.05761895201766713\n",
      "3\n",
      "   0 : 0.07324988409828466\n"
     ]
    }
   ],
   "source": [
    "for i_22 in range(4):\n",
    "    print(i_22)\n",
    "    for c in range(input_data_by_22[i_22].shape[1]):\n",
    "        tmp = input_data_by_22[i_22][:,c]\n",
    "        \n",
    "        if np.any(tmp[tmp == -999]):\n",
    "            print('  ', c, ':', len(tmp[tmp == -999]) / len(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_useless_col(x):\n",
    "    useless_cols = [[4, 5, 6, 12, 22, 23, 24, 25, 26, 27, 28], [4, 5, 6, 12, 25, 26, 27], [], []]\n",
    "    return [np.delete(x[i], useless_cols[i], 1) for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_good_data = delete_useless_col(input_data_by_22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10058,), (25000,))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb_by_22[0].shape, yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.any(only_good_data[3] == -999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def pseudo_cross_validation():\n",
    "    bests = []\n",
    "    \n",
    "    for i_22 in range(4):\n",
    "        print('starting ', i_22)\n",
    "        input_data_clean = replace_nans_with_median(only_good_data[i_22])\n",
    "        input_data_std, _, _ = standardize(input_data_clean)\n",
    "\n",
    "        times = 200\n",
    "        degrees = np.linspace(6, 13, 8).astype(int) # [1, 2, 3]\n",
    "        lambdas = np.logspace(-7, -4, 15) # [0.00001, 0.0001, 0.001, 0.01, 0.1, 1]\n",
    "\n",
    "        rmse_tr = np.zeros([times, len(degrees), len(lambdas)])\n",
    "        rmse_te = np.zeros([times, len(degrees), len(lambdas)])\n",
    "\n",
    "        for time in range(times):\n",
    "            x_tr, y_tr, x_te, y_te = split_data(input_data_std, yb_by_22[i_22], 0.8)\n",
    "\n",
    "            for i, degree in enumerate(degrees):\n",
    "                phi_tr = build_poly(x_tr, degree)\n",
    "                phi_te = build_poly(x_te, degree)\n",
    "\n",
    "                for j, lambda_ in enumerate(lambdas):\n",
    "                    try:\n",
    "                        w, _ = ridge_regression(y_tr, phi_tr, lambda_)\n",
    "\n",
    "                        rmse_tr[time, i, j] = np.sqrt(2 * compute_mse(y_tr, phi_tr, w))\n",
    "                        rmse_te[time, i, j] = np.sqrt(2 * compute_mse(y_te, phi_te, w))\n",
    "                    except:\n",
    "                        rmse_tr[time, i, j] = 1\n",
    "                        rmse_te[time, i, j] = 1\n",
    "\n",
    "            if time == 0.1 * times:\n",
    "                print('  10%')\n",
    "            if time == 0.2 * times:\n",
    "                print('  20%')\n",
    "            if time == 0.3 * times:\n",
    "                print('  30%')\n",
    "            if time == 0.4 * times:\n",
    "                print('  40%')\n",
    "            if time == 0.5 * times:\n",
    "                print('  50%')\n",
    "            if time == 0.6 * times:\n",
    "                print('  60%')\n",
    "            if time == 0.7 * times:\n",
    "                print('  70%')\n",
    "            if time == 0.8 * times:\n",
    "                print('  80%')\n",
    "            if time == 0.9 * times:\n",
    "                print('  90%')\n",
    "            if time == times - 1:\n",
    "                print(' 100%')\n",
    "        \n",
    "        pos = np.unravel_index(np.median(rmse_te, axis=0).argmin(), np.median(rmse_te, axis=0).shape)\n",
    "        bests.append((degrees[pos[0]], lambdas[pos[1]]))\n",
    "        \n",
    "    return bests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  0\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      " 100%\n",
      "starting  1\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      " 100%\n",
      "starting  2\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      " 100%\n",
      "starting  3\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      " 100%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(6, 0.0001),\n",
       " (6, 0.0001),\n",
       " (6, 7.1968567300115142e-07),\n",
       " (6, 9.9999999999999995e-08)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pseudo_cross_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bests = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bests = [(2, 0.0038566204211634724), (2, 0.03562247890262444), (3, 0.062101694189156162), (2, 0.32903445623126709)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bests = [(12, 9e-06), (7, 1.65e-05), (10, 2.42e-06), (8, 4e-05)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------\n",
    "# Creating submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(w, x_test, small=-1, big=1):\n",
    "    y_pred = x_test @ w\n",
    "    sep_val = (small + big) / 2\n",
    "    y_pred[y_pred < sep_val] = small\n",
    "    y_pred[y_pred >= sep_val] = big\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((99913, 29), (99913,))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data_full_by_22, ids_full_by_22, yb_full_by_22 = separate_by_col22(input_data_full, ids_full, yb_full)\n",
    "input_data_test_by_22, ids_test_by_22, yb_test_by_22 = separate_by_col22(input_data_test, ids_test, yb_test)\n",
    "\n",
    "input_data_full_by_22[0].shape, yb_full_by_22[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "   0 : 0.2614574679971575\n",
      "   4 : 1.0\n",
      "   5 : 1.0\n",
      "   6 : 1.0\n",
      "   12 : 1.0\n",
      "   22 : 1.0\n",
      "   23 : 1.0\n",
      "   24 : 1.0\n",
      "   25 : 1.0\n",
      "   26 : 1.0\n",
      "   27 : 1.0\n",
      "1\n",
      "   0 : 0.09751882802022077\n",
      "   4 : 1.0\n",
      "   5 : 1.0\n",
      "   6 : 1.0\n",
      "   12 : 1.0\n",
      "   25 : 1.0\n",
      "   26 : 1.0\n",
      "   27 : 1.0\n",
      "2\n",
      "   0 : 0.05859584350622283\n",
      "3\n",
      "   0 : 0.06663959574084101\n"
     ]
    }
   ],
   "source": [
    "for i_22 in range(4):\n",
    "    print(i_22)\n",
    "    for c in range(input_data_full_by_22[i_22].shape[1]):\n",
    "        tmp = input_data_full_by_22[i_22][:,c]\n",
    "        \n",
    "        if np.any(tmp[tmp == -999]):\n",
    "            print('  ', c, ':', len(tmp[tmp == -999]) / len(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good:  81875\n",
      "Bad:  18038\n",
      "Ratio:  0.8194629327514938\n",
      "Good:  60104\n",
      "Bad:  17440\n",
      "Ratio:  0.77509542969153\n",
      "Good:  40810\n",
      "Bad:  9569\n",
      "Ratio:  0.8100597471168542\n",
      "Good:  17548\n",
      "Bad:  4616\n",
      "Ratio:  0.7917343439812308\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([-1., -1.,  1., ..., -1., -1., -1.]),\n",
       " array([-1., -1., -1., ..., -1.,  1., -1.]),\n",
       " array([-1.,  1., -1., ...,  1.,  1.,  1.]),\n",
       " array([-1., -1., -1., ..., -1., -1., -1.])]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "only_good_data_full = delete_useless_col(input_data_full_by_22)\n",
    "only_good_data_test = delete_useless_col(input_data_test_by_22)\n",
    "\n",
    "for i_22 in range(4):\n",
    "    full_std, _, _ = standardize(only_good_data_full[i_22])\n",
    "    test_std, _, _ = standardize(only_good_data_test[i_22])\n",
    "    \n",
    "    phi_full = build_poly(full_std, bests[i_22][0])\n",
    "    phi_test = build_poly(test_std, bests[i_22][0])\n",
    "    \n",
    "    w, _ = ridge_regression(yb_full_by_22[i_22], phi_full, bests[i_22][1])\n",
    "    \n",
    "    yb_test_by_22[i_22] = predict(w, phi_test)\n",
    "    \n",
    "    prediction(w, phi_full, yb_full_by_22[i_22], verbose=True)\n",
    "    \n",
    "yb_test_by_22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "yb_submit = np.concatenate(yb_test_by_22)\n",
    "ids_submit = np.concatenate(ids_test_by_22)\n",
    "\n",
    "create_csv_submission(ids_submit, yb_submit, 'submission_by_cat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_data_clean = replace_nans_with_median(input_data)\n",
    "input_data_std, _, _ = standardize(input_data_clean)\n",
    "\n",
    "# y = (yb + 1) / 2\n",
    "\n",
    "phi = build_poly(input_data_std, 3)\n",
    "\n",
    "w, _ = ridge_regression(yb, phi, 0.00031)\n",
    "\n",
    "input_data_clean_test = replace_nans_with_median(input_data_test)\n",
    "input_data_std_test, _, _ = standardize(input_data_clean_test)\n",
    "\n",
    "phi_test = build_poly(input_data_std_test, 3)\n",
    "\n",
    "yb_test = predict(w, phi_test)\n",
    "create_csv_submission(ids_test, yb_test, 'submission_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(yb_test[yb_test == -1]) / len(yb_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(yb[yb == -1]) / len(yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction(w, phi, yb, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------\n",
    "# Plotting some stuff and data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\", color_codes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "copy_no_nans = replace_nans_with_mean(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "et = np.c_[yb, copy_no_nans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "et"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(et[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns that are correlated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = []\n",
    "for i in range(0, 31):\n",
    "    for j in range(i + 1, 31):\n",
    "        corr.append([i, j, df[i].corr(df[j])])\n",
    "        \n",
    "corr_a = np.array(corr)\n",
    "corr_a[np.logical_or(corr_a[:,2] > 0.8, corr_a[:,2] < -0.8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_no_nans[copy_no_nans[:,8] > 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(1, 31):\n",
    "    #sns.violinplot(x=0, y=i, data=df)\n",
    "    sns.stripplot(df[0], df[i], jitter=True)\n",
    "    sns.plt.show()\n",
    "    #print(df[0].corr(df[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grouped = df.groupby(by=[0]).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grouped.T.to_csv('goruped.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(by=[0]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
